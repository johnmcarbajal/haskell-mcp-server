#!/usr/bin/env python3
"""
MCP Connection Diagnostic Tool
Helps troubleshoot AnythingLLM + MCP integration issues
"""

import requests
import subprocess
import json
import sys
import time
from urllib.parse import urlparse

def check_port(host, port):
    """Check if a port is open"""
    import socket
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(2)
        result = sock.connect_ex((host, port))
        sock.close()
        return result == 0
    except:
        return False

def test_ollama():
    """Test Ollama connection"""
    print("ü¶ô Testing Ollama...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"  ‚úÖ Ollama running with {len(models)} models")
            for model in models[:3]:  # Show first 3 models
                print(f"     ‚Ä¢ {model['name']}")
            return True
        else:
            print(f"  ‚ùå Ollama returned status {response.status_code}")
            return False
    except Exception as e:
        print(f"  ‚ùå Ollama not accessible: {e}")
        print("     üí° Try: ollama serve")
        return False

def test_haskell_mcp():
    """Test Haskell MCP server"""
    print("‚ö° Testing Haskell MCP server...")
    try:
        # Test if stack build works
        result = subprocess.run(
            ["stack", "build"],
            cwd=".",
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            print("  ‚úÖ Haskell MCP server builds successfully")
        else:
            print("  ‚ùå Haskell build failed:")
            print(f"     {result.stderr[:200]}...")
            return False

        # Test basic MCP communication
        test_msg = '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"diagnostic","version":"1.0"}},"id":0}\n'

        process = subprocess.Popen(
            ["stack", "exec", "haskell-mcp-server-exe", "--", "--stdio"],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        stdout, stderr = process.communicate(input=test_msg, timeout=10)

        if "jsonrpc" in stdout and "result" in stdout:
            print("  ‚úÖ MCP server responds to initialize")
            return True
        else:
            print("  ‚ùå MCP server not responding properly")
            print(f"     stdout: {stdout[:100]}...")
            print(f"     stderr: {stderr[:100]}...")
            return False

    except subprocess.TimeoutExpired:
        print("  ‚ùå MCP server timed out")
        return False
    except FileNotFoundError:
        print("  ‚ùå Stack not found")
        print("     üí° Install Stack: https://docs.haskellstack.org/")
        return False
    except Exception as e:
        print(f"  ‚ùå MCP test failed: {e}")
        return False

def test_bridge():
    """Test MCP bridge"""
    print("üåâ Testing MCP bridge...")

    # Check if bridge is running
    if not check_port("localhost", 8000):
        print("  ‚ùå Bridge not running on port 8000")
        print("     üí° Start with: python anythingllm-mcp-bridge.py")
        return False

    try:
        # Test health endpoint
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"  ‚úÖ Bridge healthy with {data.get('mcp_tools', 0)} tools")
            if data.get('mcp_initialized'):
                print("     ‚úÖ MCP integration initialized")
            else:
                print("     ‚ùå MCP not initialized")
                return False
        else:
            print(f"  ‚ùå Bridge health check failed: {response.status_code}")
            return False

        # Test chat completion
        chat_data = {
            "model": "llama2",
            "messages": [{"role": "user", "content": "Hello"}]
        }

        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json=chat_data,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                print("  ‚úÖ Chat completions working")
                return True
            else:
                print("  ‚ùå Invalid chat response format")
                return False
        else:
            print(f"  ‚ùå Chat completion failed: {response.status_code}")
            print(f"     Response: {response.text[:200]}...")
            return False

    except Exception as e:
        print(f"  ‚ùå Bridge test failed: {e}")
        return False

def test_anythingllm_config():
    """Check AnythingLLM configuration"""
    print("üìö AnythingLLM Configuration Checklist:")
    print("  üìã In AnythingLLM Settings ‚Üí LLM Configuration:")
    print("     ‚Ä¢ Provider: OpenAI")
    print("     ‚Ä¢ Base URL: http://localhost:8000/v1")
    print("     ‚Ä¢ API Key: sk-anythingllm (any value works)")
    print("     ‚Ä¢ Model: llama2")
    print()
    print("  üîç If AnythingLLM is running in Docker:")
    print("     ‚Ä¢ Base URL: http://host.docker.internal:8000/v1")
    print("     ‚Ä¢ (Docker needs special host to reach localhost)")

def main():
    print("üîç MCP Connection Diagnostic Tool")
    print("=" * 50)
    print()

    results = []

    # Run all tests
    results.append(("Ollama", test_ollama()))
    results.append(("Haskell MCP", test_haskell_mcp()))
    results.append(("Bridge", test_bridge()))

    print()
    print("üìä Diagnostic Summary:")
    print("-" * 30)

    all_passed = True
    for test_name, passed in results:
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"  {test_name:<15} {status}")
        if not passed:
            all_passed = False

    print()

    if all_passed:
        print("üéâ All tests passed! Your MCP integration should work.")
        print("   If AnythingLLM still isn't connecting, check the configuration below:")
        print()
        test_anythingllm_config()
    else:
        print("‚ùå Some tests failed. Fix the issues above and try again.")
        print()
        print("üîß Common fixes:")
        print("  ‚Ä¢ Start Ollama: ollama serve")
        print("  ‚Ä¢ Build MCP server: cd haskell-mcp-server && stack build")
        print("  ‚Ä¢ Start bridge: python anythingllm-mcp-bridge.py")

    print()
    print("üÜò If you need help, share the output above!")

if __name__ == "__main__":
    main()
